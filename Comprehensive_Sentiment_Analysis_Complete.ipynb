{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Sentiment Analysis Project\n",
    "\n",
    "## A Deep Learning Approach to Social Media Sentiment Classification\n",
    "\n",
    "**Authors**: Discovery Project Team  \n",
    "**Date**: January 2025  \n",
    "**Objective**: Develop and optimize neural network architectures for sentiment analysis using multiple deep learning approaches\n",
    "\n",
    "---\n",
    "\n",
    "This comprehensive notebook implements and compares multiple neural network architectures for sentiment analysis of social media data from the Exorde dataset. We systematically progress through 12 key phases: from basic model implementation to advanced optimization techniques, incorporating insights from foundational literature in natural language processing and deep learning.\n",
    "\n",
    "**Key Features:**\n",
    "- Integration of all 41 Python files from the repository\n",
    "- Comprehensive literature review with detailed citations\n",
    "- Systematic progression through 12 structured phases\n",
    "- Multiple neural network architectures (RNN, LSTM, GRU, Transformer)\n",
    "- Advanced techniques: attention mechanisms, bidirectional processing, pre-trained embeddings\n",
    "- Extensive hyperparameter optimization and error analysis\n",
    "- Production-ready model development pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review\n",
    "\n",
    "Our approach is grounded in foundational research in natural language processing and deep learning. This section reviews five key papers that inform our architectural choices and optimization strategies.\n",
    "\n",
    "### 1. \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "\n",
    "**Citation**: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In *Advances in neural information processing systems* (pp. 5998-6008).\n",
    "\n",
    "**Key Contributions**:\n",
    "- Introduced the Transformer architecture based solely on self-attention mechanisms\n",
    "- Demonstrated superior performance to RNNs/LSTMs while enabling parallelization\n",
    "- Established multi-head attention and positional encoding as fundamental techniques\n",
    "\n",
    "**Application to Our Project**: This paper provides the theoretical foundation for our Transformer implementation. We leverage self-attention to capture long-range dependencies in social media text, implementing positional encodings and multi-head attention adapted for sentiment classification.\n",
    "\n",
    "### 2. \"Bidirectional LSTM-CRF Models for Sequence Tagging\" (Huang et al., 2015)\n",
    "\n",
    "**Citation**: Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence tagging. *arXiv preprint arXiv:1508.01991*.\n",
    "\n",
    "**Key Contributions**:\n",
    "- Demonstrated effectiveness of bidirectional processing for sequence understanding\n",
    "- Showed that backward context is crucial for linguistic meaning\n",
    "- Established bidirectional LSTMs as standard for sequence processing\n",
    "\n",
    "**Application to Our Project**: This research validates our bidirectional variants for RNN, LSTM, and GRU models. For sentiment analysis, understanding both preceding and following context is crucial, especially for phrases like \"not bad at all\" where sentiment emerges from the complete context.\n",
    "\n",
    "### 3. \"A Structured Self-Attentive Sentence Embedding\" (Lin et al., 2017)\n",
    "\n",
    "**Citation**: Lin, Z., Feng, M., Santos, C. N. D., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A structured self-attentive sentence embedding. *arXiv preprint arXiv:1703.03130*.\n",
    "\n",
    "**Key Contributions**:\n",
    "- Introduced self-attention for sentence-level representations\n",
    "- Provided interpretable attention weights showing model focus\n",
    "- Demonstrated superior performance over simple pooling strategies\n",
    "\n",
    "**Application to Our Project**: This paper directly informs our attention-enhanced models. Instead of using only final hidden states, we implement self-attention mechanisms that weight word importance, particularly valuable for sentiment analysis where specific words carry disproportionate emotional weight.\n",
    "\n",
    "### 4. \"GloVe: Global Vectors for Word Representation\" (Pennington et al., 2014)\n",
    "\n",
    "**Citation**: Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. In *Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)* (pp. 1532-1543).\n",
    "\n",
    "**Key Contributions**:\n",
    "- Introduced global matrix factorization approach to word embeddings\n",
    "- Captured both global and local statistical information\n",
    "- Demonstrated strong performance on word analogy and similarity tasks\n",
    "\n",
    "**Application to Our Project**: This research supports our use of pre-trained embeddings. GloVe embeddings provide rich semantic representations from large corpora, giving our models significant advantages over random initialization for sentiment understanding.\n",
    "\n",
    "### 5. \"Bag of Tricks for Efficient Text Classification\" (Joulin et al., 2016)\n",
    "\n",
    "**Citation**: Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T. (2016). Bag of tricks for efficient text classification. *arXiv preprint arXiv:1607.01759*.\n",
    "\n",
    "**Key Contributions**:\n",
    "- Introduced FastText for efficient text classification\n",
    "- Demonstrated that simple approaches can be highly effective\n",
    "- Showed importance of n-gram features and subword information\n",
    "\n",
    "**Application to Our Project**: While we focus on deep learning, this paper provides crucial baseline insights. It reminds us that complex models must significantly outperform simpler alternatives to justify computational cost.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Prerequisites\n",
    "\n",
    "This section imports all necessary libraries and configures the environment for our comprehensive sentiment analysis project. We systematically import all modules from our repository, ensuring compatibility and proper initialization.\n",
    "\n",
    "### 1.1 Core Deep Learning and Data Science Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for deep learning and data manipulation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility across all libraries\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Configure matplotlib and seaborn for publication-quality plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11\n",
    "\n",
    "# Check device availability and display system information\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"üöÄ System Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üîß Device: {device}\")\n",
    "print(f\"üìö PyTorch version: {torch.__version__}\")\n",
    "print(f\"üêç Python version: {sys.version.split()[0]}\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(f\"üêº Pandas version: {pd.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "    print(f\"üî• CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU available, using CPU\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Core libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Project-Specific Module Imports\n",
    "\n",
    "Here we import all the custom modules from our repository. Each import represents a different aspect of our sentiment analysis pipeline. This comprehensive import strategy ensures we have access to all 41 Python files in our repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all model architectures from our repository\n",
    "# These represent the core neural network implementations covering multiple architectures\n",
    "\n",
    "print(\"üì¶ Importing Model Architectures...\")\n",
    "from models import (\n",
    "    # Base model class - foundation for all architectures\n",
    "    BaseModel,\n",
    "    \n",
    "    # Original model architectures - basic implementations\n",
    "    RNNModel, LSTMModel, GRUModel, TransformerModel,\n",
    "    \n",
    "    # Enhanced RNN variants - addressing RNN limitations\n",
    "    DeepRNNModel,              # Multiple stacked layers\n",
    "    BidirectionalRNNModel,     # Forward and backward processing\n",
    "    RNNWithAttentionModel,     # Attention mechanism integration\n",
    "    \n",
    "    # Enhanced LSTM variants - leveraging LSTM gating mechanisms\n",
    "    StackedLSTMModel,                    # Deep hierarchical features\n",
    "    BidirectionalLSTMModel,              # Bidirectional context\n",
    "    LSTMWithAttentionModel,              # Self-attention enhancement\n",
    "    LSTMWithPretrainedEmbeddingsModel,   # Pre-trained word representations\n",
    "    \n",
    "    # Enhanced GRU variants - efficient gating with fewer parameters\n",
    "    StackedGRUModel,                     # Multiple GRU layers\n",
    "    BidirectionalGRUModel,               # Bidirectional GRU processing\n",
    "    GRUWithAttentionModel,               # Attention-enhanced GRU\n",
    "    GRUWithPretrainedEmbeddingsModel,    # Pre-trained embeddings + GRU\n",
    "    \n",
    "    # Enhanced Transformer variants - modern attention-based architectures\n",
    "    LightweightTransformerModel,         # Efficient transformer variant\n",
    "    DeepTransformerModel,                # Multi-layer transformer\n",
    "    TransformerWithPoolingModel          # Enhanced pooling strategies\n",
    ")\n",
    "\n",
    "# Count available model architectures\n",
    "model_classes = [cls for name, cls in globals().items() \n",
    "                if isinstance(cls, type) and issubclass(cls, BaseModel) and cls != BaseModel]\n",
    "\n",
    "print(f\"‚úÖ Successfully imported {len(model_classes)} model architectures\")\n",
    "print(\"üìä Available model families:\")\n",
    "print(\"   ‚Ä¢ RNN Family: 4 variants (Basic, Deep, Bidirectional, Attention)\")\n",
    "print(\"   ‚Ä¢ LSTM Family: 4 variants (Basic, Stacked, Bidirectional, Attention, Pre-trained)\")\n",
    "print(\"   ‚Ä¢ GRU Family: 4 variants (Basic, Stacked, Bidirectional, Attention, Pre-trained)\")\n",
    "print(\"   ‚Ä¢ Transformer Family: 4 variants (Basic, Lightweight, Deep, Pooling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core training and evaluation utilities\n",
    "# These modules handle the fundamental training loop, evaluation metrics, and data processing\n",
    "\n",
    "print(\"üîß Importing Core Training & Evaluation Utilities...\")\n",
    "try:\n",
    "    from train import train_model, train_model_epochs\n",
    "    print(\"  ‚úÖ Training functions: train_model, train_model_epochs\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ‚ùå Training import error: {e}\")\n",
    "\n",
    "try:\n",
    "    from evaluate import evaluate_model, evaluate_model_comprehensive\n",
    "    print(\"  ‚úÖ Evaluation functions: evaluate_model, evaluate_model_comprehensive\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ‚ùå Evaluation import error: {e}\")\n",
    "\n",
    "try:\n",
    "    from utils import simple_tokenizer, tokenize_texts\n",
    "    print(\"  ‚úÖ Utility functions: simple_tokenizer, tokenize_texts\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ‚ùå Utils import error: {e}\")\n",
    "\n",
    "try:\n",
    "    from getdata import download_exorde_sample\n",
    "    print(\"  ‚úÖ Data acquisition: download_exorde_sample\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ‚ùå Data acquisition import error: {e}\")\n",
    "\n",
    "print(\"‚úÖ Core utilities imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import advanced training and optimization modules\n",
    "# These represent our enhanced training strategies, hyperparameter optimization, and model comparison\n",
    "\n",
    "print(\"üöÄ Importing Advanced Training & Optimization Modules...\")\n",
    "\n",
    "# Import all optimization and comparison modules\n",
    "advanced_modules = {\n",
    "    'baseline_v2': 'Foundational improvements and baseline V2 implementation',\n",
    "    'enhanced_training': 'Advanced training techniques and regularization',\n",
    "    'hyperparameter_tuning': 'Grid search and hyperparameter optimization',\n",
    "    'final_hyperparameter_optimization': 'Final focused hyperparameter search',\n",
    "    'enhanced_compare_models': 'Comprehensive model comparison framework',\n",
    "    'experiment_tracker': 'Experiment logging and tracking system',\n",
    "    'compare_models': 'Basic model comparison utilities',\n",
    "    'final_model_training': 'Final model training with optimal settings'\n",
    "}\n",
    "\n",
    "imported_modules = {}\n",
    "for module_name, description in advanced_modules.items():\n",
    "    try:\n",
    "        imported_modules[module_name] = __import__(module_name)\n",
    "        print(f\"  ‚úÖ {module_name}: {description}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"  ‚ùå {module_name}: Import failed - {e}\")\n",
    "\n",
    "print(f\"‚úÖ Successfully imported {len(imported_modules)}/{len(advanced_modules)} advanced modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import analysis, visualization, and reporting modules\n",
    "# These modules provide comprehensive analysis, visualization, and final reporting capabilities\n",
    "\n",
    "print(\"üìä Importing Analysis, Visualization & Reporting Modules...\")\n",
    "\n",
    "analysis_modules = {\n",
    "    'error_analysis': 'Comprehensive error analysis and failure mode detection',\n",
    "    'visualize_models': 'Model architecture visualization and diagrams',\n",
    "    'demo_examples': 'Interactive demonstrations and example predictions',\n",
    "    'comprehensive_eval': 'Complete evaluation pipeline and metrics',\n",
    "    'final_report_generator': 'Automated final report generation',\n",
    "    'simplified_final_report': 'Streamlined reporting for key results',\n",
    "    'embedding_utils': 'Pre-trained embedding utilities and management',\n",
    "    'validate_improvements': 'Validation of model improvements and progress',\n",
    "    'test_improvements': 'Testing framework for improvement validation'\n",
    "}\n",
    "\n",
    "analysis_imported = {}\n",
    "for module_name, description in analysis_modules.items():\n",
    "    try:\n",
    "        analysis_imported[module_name] = __import__(module_name)\n",
    "        print(f\"  ‚úÖ {module_name}: {description}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"  ‚ùå {module_name}: Import failed - {e}\")\n",
    "\n",
    "print(f\"‚úÖ Successfully imported {len(analysis_imported)}/{len(analysis_modules)} analysis modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional specialized modules\n",
    "# These include quickstart utilities, testing frameworks, and specialized implementations\n",
    "\n",
    "print(\"üîß Importing Additional Specialized Modules...\")\n",
    "\n",
    "specialized_modules = {\n",
    "    'quickstart': 'Quick start utilities and examples',\n",
    "    'example': 'Example implementations and demonstrations',\n",
    "    'exorde_train_eval': 'Main training and evaluation pipeline for Exorde data',\n",
    "    'week3_implementation_demo': 'Week 3 implementation demonstrations',\n",
    "    'quick_enhanced_test': 'Quick testing for enhanced features',\n",
    "    'realistic_enhanced_test': 'Realistic testing scenarios for enhancements'\n",
    "}\n",
    "\n",
    "specialized_imported = {}\n",
    "for module_name, description in specialized_modules.items():\n",
    "    try:\n",
    "        specialized_imported[module_name] = __import__(module_name)\n",
    "        print(f\"  ‚úÖ {module_name}: {description}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"  ‚ùå {module_name}: Import failed - {e}\")\n",
    "\n",
    "print(f\"‚úÖ Successfully imported {len(specialized_imported)}/{len(specialized_modules)} specialized modules\")\n",
    "\n",
    "# Calculate total imported modules\n",
    "total_attempted = len(advanced_modules) + len(analysis_modules) + len(specialized_modules)\n",
    "total_imported = len(imported_modules) + len(analysis_imported) + len(specialized_imported)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üéØ MODULE IMPORT SUMMARY:\")\n",
    "print(f\"üì¶ Model Architectures: {len(model_classes)} classes\")\n",
    "print(f\"üîß Core Utilities: 4 modules\")\n",
    "print(f\"üöÄ Advanced Modules: {len(imported_modules)}/{len(advanced_modules)}\")\n",
    "print(f\"üìä Analysis Modules: {len(analysis_imported)}/{len(analysis_modules)}\")\n",
    "print(f\"üîß Specialized Modules: {len(specialized_imported)}/{len(specialized_modules)}\")\n",
    "print(f\"üìà Total Repository Integration: {total_imported}/{total_attempted} modules ({total_imported/total_attempted*100:.1f}%)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ All available repository modules successfully integrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Environment Configuration and Global Settings\n",
    "\n",
    "We establish global configuration parameters that will be used throughout our analysis. These settings ensure consistency across all experiments and provide a foundation for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration parameters\n",
    "# These settings control various aspects of our training and evaluation pipeline\n",
    "# Based on the repository's established patterns and best practices\n",
    "\n",
    "CONFIG = {\n",
    "    # Data settings - based on Exorde dataset characteristics\n",
    "    'SAMPLE_SIZE': 10000,        # Number of samples to download (manageable for development)\n",
    "    'EXTENDED_SAMPLE_SIZE': 50000, # Larger sample for final training\n",
    "    'TEST_SIZE': 0.2,            # Standard 80/20 train-test split\n",
    "    'VALIDATION_SIZE': 0.15,     # Validation set for hyperparameter tuning\n",
    "    'RANDOM_STATE': 42,          # Fixed seed for reproducibility\n",
    "    \n",
    "    # Model architecture settings - optimized for sentiment analysis\n",
    "    'EMBED_DIM': 64,             # Embedding dimension (balance between capacity and efficiency)\n",
    "    'HIDDEN_DIM': 64,            # Hidden layer dimension\n",
    "    'NUM_CLASSES': 3,            # Sentiment classes: Positive, Negative, Neutral\n",
    "    'NUM_HEADS': 4,              # Attention heads for Transformer (multiple perspectives)\n",
    "    'NUM_LAYERS': 2,             # Default number of layers for stacked models\n",
    "    'MAX_SEQ_LENGTH': 128,       # Maximum sequence length for padding/truncation\n",
    "    \n",
    "    # Training settings - progressive complexity\n",
    "    'BATCH_SIZE': 32,            # Batch size (balance between stability and efficiency)\n",
    "    'LARGE_BATCH_SIZE': 64,      # Larger batch for stable models\n",
    "    'LEARNING_RATE': 1e-3,       # Initial learning rate\n",
    "    'FINE_TUNE_LR': 5e-4,        # Learning rate for fine-tuning\n",
    "    'NUM_EPOCHS': 10,            # Epochs for initial experiments\n",
    "    'EXTENDED_EPOCHS': 50,       # Extended training for best models\n",
    "    'FINAL_EPOCHS': 100,         # Maximum epochs for final model\n",
    "    \n",
    "    # Regularization and optimization\n",
    "    'GRADIENT_CLIP': 1.0,        # Gradient clipping (prevent exploding gradients)\n",
    "    'WEIGHT_DECAY': 1e-4,        # L2 regularization strength\n",
    "    'DROPOUT_RATE': 0.3,         # Dropout probability\n",
    "    'PATIENCE': 10,              # Early stopping patience\n",
    "    'MIN_DELTA': 1e-4,           # Minimum improvement for early stopping\n",
    "    \n",
    "    # Hyperparameter tuning ranges\n",
    "    'HP_LEARNING_RATES': [1e-4, 5e-4, 1e-3, 2e-3],\n",
    "    'HP_BATCH_SIZES': [32, 64],\n",
    "    'HP_DROPOUT_RATES': [0.3, 0.4, 0.5],\n",
    "    'HP_WEIGHT_DECAYS': [1e-4, 5e-4, 1e-3],\n",
    "    \n",
    "    # Evaluation and deployment settings\n",
    "    'TARGET_F1': 0.75,           # Target F1 score for production readiness\n",
    "    'BASELINE_F1': 0.35,         # Expected baseline performance\n",
    "    'TOP_K_MODELS': 3,           # Number of top models for detailed analysis\n",
    "    'CONFIDENCE_THRESHOLD': 0.8,  # Threshold for high-confidence predictions\n",
    "    \n",
    "    # Paths and file management\n",
    "    'DATA_PATH': 'exorde_raw_sample.csv',\n",
    "    'MODEL_SAVE_DIR': 'saved_models',\n",
    "    'RESULTS_DIR': 'results',\n",
    "    'PLOTS_DIR': 'plots',\n",
    "    'LOGS_DIR': 'logs'\n",
    "}\n",
    "\n",
    "# Create necessary directories\n",
    "for dir_key in ['MODEL_SAVE_DIR', 'RESULTS_DIR', 'PLOTS_DIR', 'LOGS_DIR']:\n",
    "    os.makedirs(CONFIG[dir_key], exist_ok=True)\n",
    "\n",
    "# Display configuration in organized categories\n",
    "config_categories = {\n",
    "    'Data Configuration': ['SAMPLE_SIZE', 'EXTENDED_SAMPLE_SIZE', 'TEST_SIZE', 'VALIDATION_SIZE', 'RANDOM_STATE'],\n",
    "    'Model Architecture': ['EMBED_DIM', 'HIDDEN_DIM', 'NUM_CLASSES', 'NUM_HEADS', 'NUM_LAYERS', 'MAX_SEQ_LENGTH'],\n",
    "    'Training Parameters': ['BATCH_SIZE', 'LARGE_BATCH_SIZE', 'LEARNING_RATE', 'FINE_TUNE_LR', 'NUM_EPOCHS', 'EXTENDED_EPOCHS', 'FINAL_EPOCHS'],\n",
    "    'Regularization': ['GRADIENT_CLIP', 'WEIGHT_DECAY', 'DROPOUT_RATE', 'PATIENCE', 'MIN_DELTA'],\n",
    "    'Evaluation Metrics': ['TARGET_F1', 'BASELINE_F1', 'TOP_K_MODELS', 'CONFIDENCE_THRESHOLD']\n",
    "}\n",
    "\n",
    "print(\"üîß COMPREHENSIVE CONFIGURATION SETTINGS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for category, keys in config_categories.items():\n",
    "    print(f\"\\nüìä {category}:\")\n",
    "    print(\"-\" * (len(category) + 4))\n",
    "    for key in keys:\n",
    "        if key in CONFIG:\n",
    "            value = CONFIG[key]\n",
    "            if isinstance(value, float) and value < 1:\n",
    "                print(f\"  {key:20}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {key:20}: {value}\")\n",
    "\n",
    "print(f\"\\nüìÅ Directory Structure:\")\n",
    "print(\"-\" * 19)\n",
    "for dir_key in ['MODEL_SAVE_DIR', 'RESULTS_DIR', 'PLOTS_DIR', 'LOGS_DIR']:\n",
    "    print(f\"  {dir_key:15}: {CONFIG[dir_key]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Configuration loaded and directories created successfully!\")\n",
    "print(f\"üéØ Ready for systematic model development and optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Core Utilities & Model Definitions\n",
    "\n",
    "This section provides a comprehensive overview of all neural network architectures and utility functions implemented in our project. We systematically examine each model variant, explaining the architectural choices and their theoretical foundations.\n",
    "\n",
    "### 2.1 Model Architecture Analysis\n",
    "\n",
    "Our project implements four main neural network families, each with multiple variants designed to address specific challenges in sentiment analysis. Let's analyze each family systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis of all available model architectures\n",
    "# This analysis helps us understand the complexity and capabilities of each model variant\n",
    "\n",
    "def analyze_model_architecture(model_class, model_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Analyze a model architecture including parameter count, memory usage, and structure.\n",
    "    \n",
    "    This function provides detailed insights into model complexity, which helps in:\n",
    "    - Understanding computational requirements\n",
    "    - Comparing model efficiency\n",
    "    - Planning training strategies\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create model instance with standard parameters\n",
    "        model = model_class(\n",
    "            vocab_size=10000,  # Standard vocabulary size\n",
    "            embed_dim=CONFIG['EMBED_DIM'],\n",
    "            hidden_dim=CONFIG['HIDDEN_DIM'],\n",
    "            num_classes=CONFIG['NUM_CLASSES'],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Calculate parameter statistics\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Estimate memory usage (approximate)\n",
    "        memory_mb = (total_params * 4) / (1024 * 1024)  # Float32 assumption\n",
    "        \n",
    "        # Analyze layer structure\n",
    "        layers = list(model.named_children())\n",
    "        layer_info = []\n",
    "        for layer_name, layer in layers:\n",
    "            layer_params = sum(p.numel() for p in layer.parameters())\n",
    "            layer_info.append((layer_name, layer_params, str(type(layer).__name__)))\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params,\n",
    "            'memory_mb': memory_mb,\n",
    "            'layers': layer_info,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'error': str(e),\n",
    "            'success': False\n",
    "        }\n",
    "\n",
    "# Define all model variants with their specific parameters\n",
    "model_definitions = [\n",
    "    # RNN Family - Basic recurrent architectures\n",
    "    ('RNN (Basic)', RNNModel, {}),\n",
    "    ('RNN (Deep)', DeepRNNModel, {'num_layers': CONFIG['NUM_LAYERS']}),\n",
    "    ('RNN (Bidirectional)', BidirectionalRNNModel, {}),\n",
    "    ('RNN (Attention)', RNNWithAttentionModel, {}),\n",
    "    \n",
    "    # LSTM Family - Long Short-Term Memory variants\n",
    "    ('LSTM (Basic)', LSTMModel, {}),\n",
    "    ('LSTM (Stacked)', StackedLSTMModel, {'num_layers': CONFIG['NUM_LAYERS']}),\n",
    "    ('LSTM (Bidirectional)', BidirectionalLSTMModel, {}),\n",
    "    ('LSTM (Attention)', LSTMWithAttentionModel, {}),\n",
    "    ('LSTM (Pre-trained)', LSTMWithPretrainedEmbeddingsModel, {}),\n",
    "    \n",
    "    # GRU Family - Gated Recurrent Unit variants\n",
    "    ('GRU (Basic)', GRUModel, {}),\n",
    "    ('GRU (Stacked)', StackedGRUModel, {'num_layers': CONFIG['NUM_LAYERS']}),\n",
    "    ('GRU (Bidirectional)', BidirectionalGRUModel, {}),\n",
    "    ('GRU (Attention)', GRUWithAttentionModel, {}),\n",
    "    ('GRU (Pre-trained)', GRUWithPretrainedEmbeddingsModel, {}),\n",
    "    \n",
    "    # Transformer Family - Attention-based architectures\n",
    "    ('Transformer (Basic)', TransformerModel, {'num_heads': CONFIG['NUM_HEADS'], 'num_layers': CONFIG['NUM_LAYERS']}),\n",
    "    ('Transformer (Lightweight)', LightweightTransformerModel, {'num_heads': CONFIG['NUM_HEADS'], 'num_layers': CONFIG['NUM_LAYERS']}),\n",
    "    ('Transformer (Deep)', DeepTransformerModel, {'num_heads': CONFIG['NUM_HEADS'], 'num_layers': CONFIG['NUM_LAYERS']}),\n",
    "    ('Transformer (Pooling)', TransformerWithPoolingModel, {'num_heads': CONFIG['NUM_HEADS'], 'num_layers': CONFIG['NUM_LAYERS']})\n",
    "]\n",
    "\n",
    "print(\"üß† COMPREHENSIVE MODEL ARCHITECTURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìä Analyzing {len(model_definitions)} model variants...\")\n",
    "print(f\"üîß Standard configuration: vocab_size=10,000, embed_dim={CONFIG['EMBED_DIM']}, hidden_dim={CONFIG['HIDDEN_DIM']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze each model and store results\n",
    "analysis_results = []\n",
    "for model_name, model_class, kwargs in model_definitions:\n",
    "    result = analyze_model_architecture(model_class, model_name, **kwargs)\n",
    "    analysis_results.append(result)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"\\nüèóÔ∏è {result['model_name']}:\")\n",
    "        print(f\"   üìä Parameters: {result['total_params']:,} total, {result['trainable_params']:,} trainable\")\n",
    "        print(f\"   üíæ Memory: {result['memory_mb']:.2f} MB\")\n",
    "        print(f\"   üîß Layers: {len(result['layers'])} components\")\n",
    "        \n",
    "        # Show layer breakdown for complex models\n",
    "        if len(result['layers']) > 2:\n",
    "            layer_summary = ', '.join([f\"{name}({params:,})\" for name, params, _ in result['layers'][:3]])\n",
    "            if len(result['layers']) > 3:\n",
    "                layer_summary += '...'\n",
    "            print(f\"   üèóÔ∏è Structure: {layer_summary}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {result['model_name']}: Analysis failed - {result['error']}\")\n",
    "\n",
    "# Create summary statistics\n",
    "successful_analyses = [r for r in analysis_results if r['success']]\n",
    "if successful_analyses:\n",
    "    param_counts = [r['total_params'] for r in successful_analyses]\n",
    "    memory_usage = [r['memory_mb'] for r in successful_analyses]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìà ARCHITECTURE SUMMARY STATISTICS:\")\n",
    "    print(f\"   üî¢ Parameter range: {min(param_counts):,} - {max(param_counts):,}\")\n",
    "    print(f\"   üìä Average parameters: {sum(param_counts)/len(param_counts):,.0f}\")\n",
    "    print(f\"   üíæ Memory range: {min(memory_usage):.2f} - {max(memory_usage):.2f} MB\")\n",
    "    print(f\"   üéØ Models analyzed: {len(successful_analyses)}/{len(model_definitions)} successful\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ Model architecture analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}