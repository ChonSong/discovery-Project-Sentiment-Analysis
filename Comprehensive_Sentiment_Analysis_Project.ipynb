{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Sentiment Analysis Project\n",
    "\n",
    "## A Deep Learning Approach to Social Media Sentiment Classification\n",
    "\n",
    "**Authors**: Discovery Project Team  \n",
    "**Date**: January 2025  \n",
    "**Objective**: Develop and optimize neural network architectures for sentiment analysis using multiple deep learning approaches\n",
    "\n",
    "---\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This comprehensive project implements and compares multiple neural network architectures for sentiment analysis of social media data from the Exorde dataset. We systematically progress through 12 key phases: from basic model implementation to advanced optimization techniques, incorporating insights from foundational literature in natural language processing and deep learning. Our implementation includes RNN, LSTM, GRU, and Transformer architectures with various enhancements including attention mechanisms, bidirectional processing, and pre-trained embeddings.\n",
    "\n",
    "The project demonstrates a methodical approach to machine learning model development, progressing from baseline implementations to sophisticated optimized models. We achieve significant performance improvements through systematic hyperparameter tuning, architectural enhancements, and advanced training techniques, ultimately reaching competitive F1 scores on multi-class sentiment classification.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Prerequisites](#1-setup--prerequisites)\n",
    "2. [Core Utilities & Model Definitions](#2-core-utilities--model-definitions)\n",
    "3. [Data Acquisition](#3-data-acquisition)\n",
    "4. [Model Visualization](#4-model-visualization)\n",
    "5. [Enhanced Architecture Comparison](#5-enhanced-architecture-comparison)\n",
    "6. [Hyperparameter Tuning](#6-hyperparameter-tuning)\n",
    "7. [Foundational Improvements (Baseline V2)](#7-foundational-improvements-baseline-v2)\n",
    "8. [Advanced Training Demonstration](#8-advanced-training-demonstration)\n",
    "9. [Final Hyperparameter Optimization](#9-final-hyperparameter-optimization)\n",
    "10. [Comprehensive Error Analysis](#10-comprehensive-error-analysis)\n",
    "11. [Final Model Training](#11-final-model-training)\n",
    "12. [Final Report Generation](#12-final-report-generation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review\n",
    "\n",
    "Our approach is grounded in foundational research in natural language processing and deep learning. This section reviews five key papers that inform our architectural choices and optimization strategies.\n",
    "\n",
    "### 1. \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
    "\n",
    "**Citation**: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).\n",
    "\n",
    "**Key Contributions**:\n",
    "- Introduced the Transformer architecture based solely on self-attention mechanisms\n",
    "- Demonstrated superior performance to RNNs/LSTMs while enabling parallelization\n",
    "- Established the foundation for modern language models (BERT, GPT, etc.)\n",
    "\n",
    "**Application to Our Project**:\n",
    "This paper provides the theoretical foundation for our Transformer implementation. We leverage the self-attention mechanism to capture long-range dependencies in social media text, which often contains complex linguistic structures. Our implementation includes positional encodings and multi-head attention as described in the original paper, adapted for sentiment classification tasks.\n",
    "\n",
    "### 2. \"Bidirectional LSTM-CRF Models for Sequence Tagging\" (Huang et al., 2015)\n",
    "\n",
    "**Citation**: Huang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM-CRF models for sequence tagging. arXiv preprint arXiv:1508.01991.\n",
    "\n",
    "**Key Contributions**:\n",
    "- Demonstrated the effectiveness of bidirectional processing for sequence understanding\n",
    "- Showed that backward context is crucial for understanding linguistic meaning\n",
    "- Established bidirectional LSTMs as a standard for sequence processing\n",
    "\n",
    "**Application to Our Project**:\n",
    "This research validates our implementation of bidirectional variants for RNN, LSTM, and GRU models. For sentiment analysis, understanding both preceding and following context is crucial. For example, in \"The movie was not bad at all,\" the sentiment is only clear when considering the complete phrase. Our bidirectional models capture this dual-context information effectively.\n",
    "\n",
    "### 3. \"A Structured Self-Attentive Sentence Embedding\" (Lin et al., 2017)\n",
    "\n",
    "**Citation**: Lin, Z., Feng, M., Santos, C. N. D., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130.\n",
    "\n",
    "**Key Contributions**:\n",
    "- Introduced self-attention for sentence-level representations\n",
    "- Provided interpretable attention weights showing model focus\n",
    "- Demonstrated superior performance over simple pooling strategies\n",
    "\n",
    "**Application to Our Project**:\n",
    "This paper directly informs our attention-enhanced RNN, LSTM, and GRU models. Instead of using only the final hidden state, we implement self-attention mechanisms that weight the importance of each word in the sequence. This approach is particularly valuable for sentiment analysis where specific words or phrases carry disproportionate emotional weight.\n",
    "\n",
    "### 4. \"GloVe: Global Vectors for Word Representation\" (Pennington et al., 2014)\n",
    "\n",
    "**Citation**: Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).\n",
    "\n",
    "**Key Contributions**:\n",
    "- Introduced global matrix factorization approach to word embeddings\n",
    "- Captured both global and local statistical information\n",
    "- Demonstrated strong performance on word analogy and similarity tasks\n",
    "\n",
    "**Application to Our Project**:\n",
    "This research supports our use of pre-trained embeddings to initialize our models. GloVe embeddings provide rich semantic representations learned from large corpora, giving our models a significant head start compared to random initialization. This is especially important for sentiment analysis where semantic relationships between words are crucial for understanding emotional nuances.\n",
    "\n",
    "### 5. \"Bag of Tricks for Efficient Text Classification\" (Joulin et al., 2016)\n",
    "\n",
    "**Citation**: Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T. (2016). Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759.\n",
    "\n",
    "**Key Contributions**:\n",
    "- Introduced FastText for efficient text classification\n",
    "- Demonstrated that simple approaches can be highly effective\n",
    "- Showed the importance of n-gram features and subword information\n",
    "\n",
    "**Application to Our Project**:\n",
    "While we focus on deep learning approaches, this paper provides important baseline insights. It reminds us that complex models must significantly outperform simpler alternatives to justify their computational cost. We use this perspective to validate that our neural networks provide meaningful improvements over traditional bag-of-words approaches.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Prerequisites\n",
    "\n",
    "This section imports all necessary libraries and configures the environment for our comprehensive sentiment analysis project. We systematically import all modules from our repository, ensuring compatibility and proper initialization.\n",
    "\n",
    "### 1.1 Core Deep Learning and Data Science Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for deep learning and data manipulation\n",
    "# Initialize execution tracking to ensure proper sequential execution\n",
    "_notebook_execution_state = {'cells_executed': set(), 'current_cell': 1}\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "print(f\"📚 PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎮 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "\n",
    "# Mark this cell as executed\n",
    "_notebook_execution_state['cells_executed'].add(1)\n",
    "print(\"\\n✅ Cell 1 executed successfully - Core libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Project-Specific Module Imports\n",
    "\n",
    "Here we import all the custom modules from our repository. Each import represents a different aspect of our sentiment analysis pipeline, from data acquisition to model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all model architectures from our repository\n",
    "# These represent the core neural network implementations\n",
    "# Added error handling to ensure robust execution\n",
    "\n",
    "# Check if previous cells were executed\n",
    "if '_notebook_execution_state' not in globals() or 1 not in _notebook_execution_state['cells_executed']:\n",
    "    print(\"⚠️  Warning: Please run Cell 1 (Core Libraries) first!\")\n",
    "    print(\"💡 Tip: Execute cells in sequential order for best results.\")\n",
    "\n",
    "try:\n",
    "    from models import (\n",
    "        # Base model class\n",
    "        BaseModel,\n",
    "        \n",
    "        # Original model architectures\n",
    "        RNNModel, LSTMModel, GRUModel, TransformerModel,\n",
    "        \n",
    "        # Enhanced RNN variants\n",
    "        DeepRNNModel, BidirectionalRNNModel, RNNWithAttentionModel,\n",
    "        \n",
    "        # Enhanced LSTM variants\n",
    "        StackedLSTMModel, BidirectionalLSTMModel, LSTMWithAttentionModel, LSTMWithPretrainedEmbeddingsModel,\n",
    "        \n",
    "        # Enhanced GRU variants\n",
    "        StackedGRUModel, BidirectionalGRUModel, GRUWithAttentionModel, GRUWithPretrainedEmbeddingsModel,\n",
    "        \n",
    "        # Enhanced Transformer variants\n",
    "        LightweightTransformerModel, DeepTransformerModel, TransformerWithPoolingModel\n",
    "    )\n",
    "    \n",
    "    # Verify models are properly imported by checking BaseModel\n",
    "    if 'BaseModel' not in locals():\n",
    "        raise ImportError(\"BaseModel not found in imports\")\n",
    "    \n",
    "    # Count available model variants\n",
    "    model_variants = [cls for cls in globals().values() \n",
    "                     if isinstance(cls, type) and hasattr(cls, '__bases__') \n",
    "                     and any(hasattr(base, '__name__') and 'BaseModel' in base.__name__ for base in cls.__bases__)]\n",
    "    \n",
    "    print(\"✅ Successfully imported all model architectures\")\n",
    "    print(f\"📊 Total model variants available: {len(model_variants)}\")\n",
    "    \n",
    "    # Mark this cell as executed\n",
    "    _notebook_execution_state['cells_executed'].add(2)\n",
    "    print(\"✅ Cell 2 executed successfully - Model architectures imported\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing models: {e}\")\n",
    "    print(\"⚠️  Please ensure all model files are available and PyTorch is installed\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected error during model import: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core training and evaluation utilities\n",
    "# These modules handle the training loop, evaluation metrics, and data processing\n",
    "# Added error handling to ensure robust execution\n",
    "\n",
    "# Check execution order\n",
    "if '_notebook_execution_state' not in globals() or 2 not in _notebook_execution_state['cells_executed']:\n",
    "    print(\"⚠️  Warning: Please run Cell 2 (Model Architectures) first!\")\n",
    "\n",
    "try:\n",
    "    from train import train_model, train_model_epochs\n",
    "    print(\"  ✅ Training functions: train_model, train_model_epochs\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ❌ Training import error: {e}\")\n",
    "    print(\"  ⚠️  Please ensure train.py is available\")\n",
    "\n",
    "try:\n",
    "    from evaluate import evaluate_model, evaluate_model_comprehensive\n",
    "    print(\"  ✅ Evaluation functions: evaluate_model, evaluate_model_comprehensive\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ❌ Evaluation import error: {e}\")\n",
    "    print(\"  ⚠️  Please ensure evaluate.py is available\")\n",
    "\n",
    "try:\n",
    "    from utils import simple_tokenizer, tokenize_texts\n",
    "    print(\"  ✅ Utility functions: simple_tokenizer, tokenize_texts\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ❌ Utils import error: {e}\")\n",
    "    print(\"  ⚠️  Please ensure utils.py is available\")\n",
    "\n",
    "try:\n",
    "    from getdata import download_exorde_sample\n",
    "    print(\"  ✅ Data acquisition: download_exorde_sample\")\n",
    "except ImportError as e:\n",
    "    print(f\"  ❌ Data acquisition import error: {e}\")\n",
    "    print(\"  ⚠️  Please ensure getdata.py is available\")\n",
    "\n",
    "print(\"\\n✅ Core utilities imported successfully!\")\n",
    "\n",
    "# Mark this cell as executed\n",
    "_notebook_execution_state['cells_executed'].add(3)\n",
    "print(\"✅ Cell 3 executed successfully - Core utilities imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import advanced training and optimization modules\n",
    "# These represent our enhanced training strategies and optimization techniques\n",
    "import baseline_v2\n",
    "import enhanced_training\n",
    "import hyperparameter_tuning\n",
    "import final_hyperparameter_optimization\n",
    "import enhanced_compare_models\n",
    "import experiment_tracker\n",
    "\n",
    "print(\"✅ Successfully imported advanced training and optimization modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import analysis and visualization modules\n",
    "# These modules provide comprehensive analysis and visualization capabilities\n",
    "import error_analysis\n",
    "import visualize_models\n",
    "import demo_examples\n",
    "import comprehensive_eval\n",
    "import final_report_generator\n",
    "import simplified_final_report\n",
    "\n",
    "print(\"✅ Successfully imported analysis and visualization modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional utility and specialized modules\n",
    "# These modules provide specific functionalities for embeddings, comparisons, and testing\n",
    "import embedding_utils\n",
    "import compare_models\n",
    "import final_model_training\n",
    "import validate_improvements\n",
    "import test_improvements\n",
    "\n",
    "print(\"✅ Successfully imported additional utility modules\")\n",
    "print(\"🎯 All repository modules successfully loaded and ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Environment Configuration and Global Settings\n",
    "\n",
    "We establish global configuration parameters that will be used throughout our analysis. These settings ensure consistency across all experiments and provide a foundation for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration parameters\n",
    "# These settings control various aspects of our training and evaluation pipeline\n",
    "\n",
    "CONFIG = {\n",
    "    # Data settings\n",
    "    'SAMPLE_SIZE': 10000,  # Number of samples to download from Exorde dataset\n",
    "    'TEST_SIZE': 0.2,      # Proportion of data for testing\n",
    "    'RANDOM_STATE': 42,    # Random seed for reproducibility\n",
    "    \n",
    "    # Model architecture settings\n",
    "    'EMBED_DIM': 64,       # Embedding dimension\n",
    "    'HIDDEN_DIM': 64,      # Hidden layer dimension\n",
    "    'NUM_CLASSES': 3,      # Number of sentiment classes (Positive, Negative, Neutral)\n",
    "    'NUM_HEADS': 4,        # Number of attention heads for Transformer\n",
    "    'NUM_LAYERS': 2,       # Number of layers for stacked models\n",
    "    \n",
    "    # Training settings\n",
    "    'BATCH_SIZE': 32,      # Batch size for training\n",
    "    'LEARNING_RATE': 1e-3, # Initial learning rate\n",
    "    'NUM_EPOCHS': 10,      # Number of training epochs for initial experiments\n",
    "    'EXTENDED_EPOCHS': 50, # Number of epochs for extended training\n",
    "    'GRADIENT_CLIP': 1.0,  # Gradient clipping value\n",
    "    \n",
    "    # Optimization settings\n",
    "    'WEIGHT_DECAY': 1e-4,  # L2 regularization strength\n",
    "    'DROPOUT_RATE': 0.3,   # Dropout probability\n",
    "    'PATIENCE': 10,        # Early stopping patience\n",
    "    \n",
    "    # Evaluation settings\n",
    "    'TARGET_F1': 0.75,     # Target F1 score for deployment readiness\n",
    "    'TOP_K_MODELS': 3,     # Number of top models to analyze in detail\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "print(\"🔧 Global Configuration Settings:\")\n",
    "print(\"=\" * 50)\n",
    "for category in ['Data', 'Model', 'Training', 'Optimization', 'Evaluation']:\n",
    "    print(f\"\\n{category} Settings:\")\n",
    "    category_keys = [k for k in CONFIG.keys() if category.upper() in k or \n",
    "                     (category == 'Data' and k in ['SAMPLE_SIZE', 'TEST_SIZE', 'RANDOM_STATE']) or\n",
    "                     (category == 'Model' and k in ['EMBED_DIM', 'HIDDEN_DIM', 'NUM_CLASSES', 'NUM_HEADS', 'NUM_LAYERS']) or\n",
    "                     (category == 'Training' and k in ['BATCH_SIZE', 'LEARNING_RATE', 'NUM_EPOCHS', 'EXTENDED_EPOCHS', 'GRADIENT_CLIP']) or\n",
    "                     (category == 'Optimization' and k in ['WEIGHT_DECAY', 'DROPOUT_RATE', 'PATIENCE']) or\n",
    "                     (category == 'Evaluation' and k in ['TARGET_F1', 'TOP_K_MODELS'])]\n",
    "    \n",
    "    for key in category_keys:\n",
    "        print(f\"  {key}: {CONFIG[key]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Validate configuration completeness\n",
    "required_keys = ['EMBED_DIM', 'HIDDEN_DIM', 'NUM_CLASSES', 'VOCAB_SIZE']\n",
    "missing_keys = [key for key in required_keys if key not in CONFIG]\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"⚠️  Warning: Missing required CONFIG keys: {missing_keys}\")\n",
    "    # Add default values for missing keys\n",
    "    if 'VOCAB_SIZE' not in CONFIG:\n",
    "        CONFIG['VOCAB_SIZE'] = 10000  # Default vocabulary size\n",
    "        print(f\"  ✅ Added default VOCAB_SIZE: {CONFIG['VOCAB_SIZE']}\")\n",
    "\n",
    "# Validate that dependencies are available\n",
    "dependencies_check = {\n",
    "    'Models available': 'BaseModel' in globals(),\n",
    "    'Training functions available': 'train_model' in globals(),\n",
    "    'Evaluation functions available': 'evaluate_model' in globals(),\n",
    "    'Utility functions available': 'simple_tokenizer' in globals()\n",
    "}\n",
    "\n",
    "print(\"\\n🔍 Dependency Check:\")\n",
    "for check_name, check_result in dependencies_check.items():\n",
    "    status = \"✅\" if check_result else \"❌\"\n",
    "    print(f\"  {status} {check_name}\")\n",
    "\n",
    "all_dependencies_ok = all(dependencies_check.values())\n",
    "if all_dependencies_ok:\n",
    "    print(\"\\n✅ Configuration loaded successfully!\")\n",
    "    print(\"✅ All dependencies are available!\")\n",
    "    print(\"✅ Ready to proceed with model development!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Some dependencies are missing. Please run previous cells first.\")\n",
    "    print(\"💡 Tip: Run cells in order from the top of the notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}